{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7549ae00",
   "metadata": {},
   "source": [
    "## Vision Transformer (ViT) 预训练模型微调配置\n",
    "\n",
    "本模块配置了一个基于 **ViT-B/16 (Base/16)** 预训练权重的深度学习模型，并采用**分层微调策略**以适应新的分类任务。\n",
    "\n",
    "### 策略说明\n",
    "\n",
    "1.  **加载模型：** 使用 PyTorch `torchvision.models` 加载 ViT-B/16 预训练模型。\n",
    "2.  **参数冻结：** 默认**冻结所有** ViT 编码器层的参数，以保持预训练获得的通用特征知识（特征提取）。\n",
    "3.  **解冻高层：** 仅**解冻最后一个 Transformer Block**（索引 11），允许模型对最高级特征进行微调，以适应特定任务。\n",
    "4.  **替换分类头：** 移除了原有的分类头，替换为一个包含 **ReLU 激活**和 **Dropout (0.4)** 的新 **`nn.Sequential`** 分类器，其输出维度匹配目标类别数（`num_classes`）。\n",
    "\n",
    "### 参数概览\n",
    "\n",
    "| 统计项 | 数值 | 目的 |\n",
    "| :--- | :--- | :--- |\n",
    "| 模型总参数量 | 约 87.17 M | ViT-B/16 的标准参数量。 |\n",
    "| **可训练参数量** | **约 7.69 M** | 仅包括最后一个 Block 和新分类头，显著减少了训练负担。 |\n",
    "| 分类头参数量 | 约 0.60 M | 新添加的、需要从头学习的参数。 |\n",
    "\n",
    "注：模型需要的输入大小为 Image_size = (224,224)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc0c0d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torchvision.models import vit_b_16, ViT_B_16_Weights\n",
    "\n",
    "# 参数设置\n",
    "num_classes = 10 # 分类数量\n",
    "# 检查是否有可用的GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 使用效果最好的 ViT-B/16 预训练权重版本\n",
    "model = vit_b_16(weights=ViT_B_16_Weights.DEFAULT)\n",
    "# 冻结所有参数 (特征提取阶段)\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "# 解冻最后一个 Block (索引 11) 的参数\n",
    "for param in model.encoder.layers[-1].parameters():\n",
    "    param.requires_grad = True\n",
    "print(\"注意：已解冻最后一个 Transformer 编码器块的参数。\")\n",
    "\n",
    "# 获取 ViT 分类头 (model.head) 的输入特征数\n",
    "# 对于 ViT-B/16，这个值是 768\n",
    "num_ftrs = 768\n",
    "\n",
    "# 修改分类头 (model.head) 以适应你的分类任务\n",
    "# ViT 的分类头就是模型在 `cls` token 上接的一个全连接层\n",
    "model.head = nn.Sequential(\n",
    "    # 第一层从 ViT 的输出 (768) 开始\n",
    "    nn.Linear(num_ftrs, 768),  # 注意：这里我们使用 2048 来匹配你 ResNet 示例中的结构\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.4),\n",
    "    \n",
    "    # 输出层：连接到你的类别数\n",
    "    nn.Linear(768, num_classes)\n",
    ")\n",
    "\n",
    "# 将模型移动到GPU（如果可用）\n",
    "model.to(device)\n",
    "\n",
    "# 打印模型结构\n",
    "print(model)\n",
    "\n",
    "# 打印可训练的参数数量 (用于确认冻结策略是否生效)\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"\\n模型总参数量: {total_params / 1e6:.2f} M\")\n",
    "print(f\"可训练参数量: {trainable_params / 1e6:.2f} M\")\n",
    "print(f\"微调层参数量 (model.head): {sum(p.numel() for p in model.head.parameters()) / 1e6:.2f} M\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19741e3d",
   "metadata": {},
   "source": [
    "## ResNet50 预训练模型微调配置\n",
    "\n",
    "本模块配置了一个基于 **ResNet50** 预训练权重的深度学习模型，采用**分层微调 (Partial Fine-Tuning)** 策略，以高效适应新的 $\\mathbf{N=10}$ 类别分类任务。\n",
    "\n",
    "### 策略说明\n",
    "\n",
    "1.  **加载模型：** 使用 PyTorch `torchvision.models` 加载 ResNet50 预训练权重。\n",
    "2.  **参数冻结：** 默认**冻结大部分参数**（包括 `layer1`、`layer2`、`layer3`），保留浅层和中层预训练获得的通用特征。\n",
    "3.  **解冻高层：** 仅**解冻最后一个卷积块 (`model.layer4`)** 的参数，允许模型对最高级特征进行微调。\n",
    "4.  **替换分类头：** 移除了原有的全连接层 (`model.fc`)，替换为一个包含两层线性层、ReLU 激活和 $\\mathbf{40\\%}$ Dropout 的新分类器结构。输入特征数固定为 $\\mathbf{2048}$。\n",
    "\n",
    "### 参数概览 (基于代码输出)\n",
    "\n",
    "| 统计项 | 描述 |\n",
    "| :--- | :--- |\n",
    "| 模型总参数量 | 约 25.56 M |\n",
    "| **可训练参数量** | 仅包括 `layer4` 和新的分类头 (`model.fc`) |\n",
    "| **微调层参数量** | 仅新分类头 (`model.fc`) 的参数量 |\n",
    "\n",
    "**核心：** 这种策略（冻结大部分 + 微调高层和分类头）旨在利用预训练的鲁棒特征，同时最大限度地减少训练负担和过拟合的风险。\n",
    "\n",
    "注：模型需要的输入大小为 Image_size = (224,224)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d30d034",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import resnet50, ResNet50_Weights\n",
    "\n",
    "# 前置参数\n",
    "num_classes = 10 # 类别数\n",
    "# 检查是否有可用的GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 加载预训练的效果最好的resnet50权重版本\n",
    "model = resnet50(weights=ResNet50_Weights.DEFAULT)\n",
    "\n",
    "# 冻结所有参数\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "for param in model.layer4.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "# 获取 ResNet50 最后一个全连接层 (model.fc) 的输入特征数\n",
    "# 对于 ResNet50，这个值固定是 2048\n",
    "num_ftrs = model.fc.in_features \n",
    "\n",
    "# 修改分类层 (model.fc) 以适应你的分类任务\n",
    "# 这里使用你提供的更复杂的分类器结构作为示例\n",
    "model.fc = nn.Sequential(\n",
    "    # 第一层从 ResNet50 的输出 (2048) 开始\n",
    "    nn.Linear(num_ftrs, 2048),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.4),\n",
    "    \n",
    "    # 输出层：连接到你的类别数\n",
    "    nn.Linear(2048 , num_classes)\n",
    ")\n",
    "\n",
    "# 将模型移动到GPU（如果可用）\n",
    "model.to(device)\n",
    "\n",
    "# 打印模型结构\n",
    "print(model)\n",
    "\n",
    "# 打印可训练的参数数量 (用于确认冻结策略是否生效)\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"\\n模型总参数量: {total_params / 1e6:.2f} M\")\n",
    "print(f\"可训练参数量: {trainable_params / 1e6:.2f} M\")\n",
    "print(f\"微调层参数量 (model.head): {sum(p.numel() for p in model.head.parameters()) / 1e6:.2f} M\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
